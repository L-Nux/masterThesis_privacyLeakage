##### CONTENT #####
(A) General info on NER tagging
(B) Extraction of sensitive data + Query patterns
(C) Information leakage in LMs
(D) Differential Privacy
(E) Knowledge Databases
(F) Data Privacy
(O) BERT


###(A) General info on NER tagging###
(A1)
title: A survey on Deep Learning for NER
author(s): Lie et al.
year: 2020
abstract: Named entity recognition (NER) is the task to identify mentions of rigid designators from text belonging to predefined
semantic types such as person, location, organization etc. NER always serves as the foundation for many natural language
applications such as question answering, text summarization, and machine translation. Early NER systems got a huge success in
achieving good performance with the cost of human engineering in designing domain-specific features and rules. In recent years, deep
learning, empowered by continuous real-valued vector representations and semantic composition through nonlinear processing, has
been employed in NER systems, yielding stat-of-the-art performance. In this paper, we provide a comprehensive review on existing
deep learning techniques for NER. We first introduce NER resources, including tagged NER corpora and off-the-shelf NER tools. Then,
we systematically categorize existing works based on a taxonomy along three axes: distributed representations for input, context
encoder, and tag decoder. Next, we survey the most representative methods for recent applied techniques of deep learning in new
NER problem settings and applications. Finally, we present readers with the challenges faced by NER systems and outline future
directions in this area.
bibtex:
@ARTICLE{9039685,
  author={Li, Jing and Sun, Aixin and Han, Jianglei and Li, Chenliang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Deep Learning for Named Entity Recognition}, 
  year={2022},
  volume={34},
  number={1},
  pages={50-70},
  doi={10.1109/TKDE.2020.2981314}}
content: definition and history of NER; List of annotated datasets for English NER; distinction between traditional and DL approaches to NER;
---
(A2)
title: A Survey on Recent Named Entity Recognition and
Relationship Extraction Techniques on Clinical Texts
author(s): Bose et al.
year: 2021
abstract: Significant growth in Electronic Health Records (EHR) over the last decade has provided
an abundance of clinical text that is mostly unstructured and untapped. This huge amount of
clinical text data has motivated the development of new information extraction and text mining
techniques. Named Entity Recognition (NER) and Relationship Extraction (RE) are key components
of information extraction tasks in the clinical domain. In this paper, we highlight the present status
of clinical NER and RE techniques in detail by discussing the existing proposed NLP models for the
two tasks and their performances and discuss the current challenges. Our comprehensive survey on
clinical NER and RE encompass current challenges, state-of-the-art practices, and future directions in
information extraction from clinical text. This is the first attempt to discuss both of these interrelated
topics together in the clinical context. We identified many research articles published based on
different approaches and looked at applications of these tasks. We also discuss the evaluation metrics
that are used in the literature to measure the effectiveness of the two these NLP methods and future
research directions.
bibtex:
@Article{app11188319,
AUTHOR = {Bose, Priyankar and Srinivasan, Sriram and Sleeman, William C. and Palta, Jatinder and Kapoor, Rishabh and Ghosh, Preetam},
TITLE = {A Survey on Recent Named Entity Recognition and Relationship Extraction Techniques on Clinical Texts},
JOURNAL = {Applied Sciences},
VOLUME = {11},
YEAR = {2021},
NUMBER = {18},
ARTICLE-NUMBER = {8319},
URL = {https://www.mdpi.com/2076-3417/11/18/8319},
ISSN = {2076-3417},
ABSTRACT = {Significant growth in Electronic Health Records (EHR) over the last decade has provided an abundance of clinical text that is mostly unstructured and untapped. This huge amount of clinical text data has motivated the development of new information extraction and text mining techniques. Named Entity Recognition (NER) and Relationship Extraction (RE) are key components of information extraction tasks in the clinical domain. In this paper, we highlight the present status of clinical NER and RE techniques in detail by discussing the existing proposed NLP models for the two tasks and their performances and discuss the current challenges. Our comprehensive survey on clinical NER and RE encompass current challenges, state-of-the-art practices, and future directions in information extraction from clinical text. This is the first attempt to discuss both of these interrelated topics together in the clinical context. We identified many research articles published based on different approaches and looked at applications of these tasks. We also discuss the evaluation metrics that are used in the literature to measure the effectiveness of the two these NLP methods and future research directions.},
DOI = {10.3390/app11188319}
}
content: definition of Clinical NER and Relationship Extraction (RE); overview on Clinical NER research articles;
---
(A3) 
title: Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition
auhtor(s): sang, De Meulder
year: 2003
abstract: We describe the CoNLL-2003 shared task:
language-independent named entity recognition.
We give background information on
the data sets (English and German) and
the evaluation method, present a general
overview of the systems that have taken
part in the task and discuss their performance.
bibtex:
@article{DBLP:journals/corr/cs-CL-0306050,
  author    = {Erik F. Tjong Kim Sang and
               Fien De Meulder},
  title     = {Introduction to the CoNLL-2003 Shared Task: Language-Independent Named
               Entity Recognition},
  journal   = {CoRR},
  volume    = {cs.CL/0306050},
  year      = {2003},
  url       = {http://arxiv.org/abs/cs/0306050},
  timestamp = {Fri, 10 Jan 2020 12:58:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/cs-CL-0306050.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
- content:
+ 4 types of named entities: persons PER, locations LOC, organizations ORG, names of miscellaneous entities MISC
+ dataset consists of 8 files covering two languages (Eng and Ger)
+ English data was taken from the Reuters news corpus between aug 1996 - aug 1997
+ German data was taken from the ECI Multilingual Text Corpus  (newspaper Frankfurter Rundschau) from Sep-Dec 1992
+ English data was tagged and chunked by the memory-based MBT tagger (Daelemans et als., 2002)
+ NER was done by hand following MUC conventions (Chinchor et al., 1999)
+ Table 2: number of named entities per data file
+ data format: all data files contain one word/line: word | POS-tag | chunk-tag | NER-tag
+ O...outside of NE; I...inside of NE; B...immediately next to (--> IOB scheme by Ramshaw and Marcus 1995)
---
(A4)
title: Named Entity Recognition and Classification on Historical Documents: A Survey
year: 2021
bibtex: @article{EhrmannMaud2021NERa,
year = {2021},
title = {Named Entity Recognition and Classification on Historical Documents: A Survey},
copyright = {http://creativecommons.org/licenses/by-sa/4.0},
language = {eng},
author = {Ehrmann, Maud and Hamdi, Ahmed and Pontes, Elvys Linhares and Romanello, Matteo and Doucet, Antoine},
abstract = {After decades of massive digitisation, an unprecedented amount of historical
documents is available in digital format, along with their machine-readable
texts. While this represents a major step forward with respect to preservation
and accessibility, it also opens up new opportunities in terms of content
mining and the next fundamental challenge is to develop appropriate
technologies to efficiently search, retrieve and explore information from this
'big data of the past'. Among semantic indexing opportunities, the recognition
and classification of named entities are in great demand among humanities
scholars. Yet, named entity recognition (NER) systems are heavily challenged
with diverse, historical and noisy inputs. In this survey, we present the array
of challenges posed by historical documents to NER, inventory existing
resources, describe the main approaches deployed so far, and identify key
priorities for future developments.},
}
content:
---
(A5)
title: Design challenges and misconceptions in named entity recognition
year: 2009
bibtex:
@inproceedings{ratinov2009design,
  title={Design challenges and misconceptions in named entity recognition},
  author={Ratinov, Lev and Roth, Dan},
  booktitle={Proceedings of the thirteenth conference on computational natural language learning (CoNLL-2009)},
  pages={147--155},
  year={2009}
}
content:
---
(A6)
title: Pre-trained models for natural language processing: A survey
year: 2020
bibtex:
@article{QiuXiPeng2020Pmfn,
year = {2020},
title = {Pre-trained models for natural language processing: A survey},
copyright = {Science China Press and Springer-Verlag GmbH Germany, part of Springer Nature 2020},
language = {eng},
address = {Beijing},
author = {Qiu, XiPeng and Sun, TianXiang and Xu, YiGe and Shao, YunFan and Dai, Ning and Huang, XuanJing},
keywords = {Engineering ; Engineering Multidisciplinary ; Materials Science ; Materials Science Multidisciplinary ; Natural language processing ; Review ; Science & Technology ; Taxonomy ; Technology},
issn = {1674-7321},
abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy from four different perspectives. Next, we describe how to adapt the knowledge of PTMs to downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
journal = {Science China. Technological sciences},
pages = {1872--1897},
volume = {63},
publisher = {Science China Press},
number = {10},
}
content:
+ p.1872: general description of neural LMs and pre-trained LMs
+ p.1881: linguistic and world knowledge of BERT;
+ p.1889: discussion of NER;
+ p.1890: adversarial attack on fine-tuned BERT
---

###(B) Extraction of sensitive data + Query patterns###
(B1)
title: Are Clinical BERT Models Privacy Preserving?
author(s): Vakili, Dalianis
year: 2021
abstract: Language models may be trained on data that contain personal
information, such as clinical data. Such sensitive data
must not leak for privacy reasons. This article explores
whether BERT models trained on clinical data are susceptible
to training data extraction attacks.
Multiple large sets of sentences generated from the model
with top-k sampling and nucleus sampling are studied. The
sentences are examined to determine the degree to which
they contain information associating patients with their conditions.
The sentence sets are then compared to determine if
there is a correlation between the degree of privacy leaked and
the linguistic quality attained by each generation technique.
We find that the relationship between linguistic quality and
privacy leakage is weak and that the risk of a successful training
data extraction attack on a BERT-based model is small.
bibtex:
@inproceedings{VakiliThomas2021ACBM,
year = {2021},
title = {Are Clinical BERT Models Privacy Preserving? The Difficulty of Extracting Patient-Condition Associations},
language = {eng},
author = {Vakili, Thomas and Dalianis, Hercules},
keywords = {Computer and Information Sciences ; Computer and Systems Sciences ; Data- och informationsvetenskap ; data- och systemvetenskap ; language models ; Language Technology (Computational Linguistics) ; natural language generation ; natural language processing ; Natural Sciences ; Naturvetenskap ; nlp ; privacy preserving machine learning ; Språkteknologi (språkvetenskaplig databehandling) ; transformers},
series = {CEUR Workshop Proceedings},
abstract = {Language models may be trained on data that contain personal information, such as clinical data. Such sensitive data must not leak for privacy reasons. This article explores whether BERT models trained on clinical data are susceptible to training data extraction attacks. Multiple large sets of sentences generated from the model with top-k sampling and nucleus sampling are studied. The sentences are examined to determine the degree to which they contain information associating patients with their conditions. The sentence sets are then compared to determine if there is a correlation between the degree of privacy leaked and the linguistic quality attained by each generation technique. We find that the relationship between linguistic quality and privacy leakage is weak and that the risk of a successful training data extraction attack on a BERT-based model is small.},
booktitle = {Proceedings of the AAAI 2021 Fall Symposium on Human Partnership with Medical AI},
}
model: clinical BERT
dataset: MIMIC-III (patient-related text corpus); This is a large and freely available dataset consisting
of de-identified clinical data of more than 40,000 patients who stayed at the Beth Israel
Deaconess Medical Center between 2001 and 2012. This dataset also consists of freetext
notes, besides also providing a demo dataset with information for 100 patients.
methods:
- text generation with top-k sampling and nucleus sampling
- NER tagger to locate PER
goal: detect names and correlate them with medical conditions
content:
- p.3: detecting names: many false positives (Max); many of the names found are not part of the MIMIC-III corpus, and have likely been learned in the earlier pre-training of BERT;
- p.5: suggestions for protecting privacy: 
a) homorphic encryption (not user-friendly)
b) pseudonymize data before training (worse performance)
- p.6: model only used lowercase tokens to train --> harder to find entities such as names; interesting: investigating the impact of lowercasing;
- unclear what risks are acceptable from a legal perspective (cf. GDPR)
-results: the risk of successfully sampling sensitive data from a BERT-based model is much smaller when compared to GPT-2;
---
(B2)
title: Discovery of sensitive data with Natural Language Processing (Master thesis)
author(s): Dias
year: 2019
abstract: The process of protecting sensitive data is continually growing and becoming increasingly important,
especially as a result of the directives and laws imposed by the European Union. The effort
to create automatic systems is continuous, but in most cases, the processes behind them are
still manual or semi-automatic. In this work, we have developed a component that can extract
and classify sensitive data, from unstructured text information in European Portuguese. The
objective was to create a system that allows organizations to understand their data and comply
with legal and security purposes. We studied a hybrid approach to the problem of Named
Entities Recognition for the Portuguese language. This approach combines several techniques
such as rule-based/lexical-based models, machine learning algorithms and neural networks. The
rule-based and lexical-based approaches were used only for a set of specific classes. For the remaining
classes of entities, SpaCy and Stanford NLP tools were tested, two statistical models –
Conditional Random Fields and Random Forest – were implemented and, finally, a Bidirectional-
LSTM approach as experimented. The best results were achieved with the Stanford NER model
(86.41%), from the Stanford NLP tool. Regarding the statistical models, we realized that Conditional
Random Fields is the one that can obtain the best results, with a f1-score of 65.50%. With
the Bi-LSTM approach, we have achieved a result of 83.01%. The corpora used for training and
testing were HAREM Golden Collection, SIGARRA News Corpus and DataSense NER Corpus. 
datasets: European Portuguese --> HAREM consists of a set of 129 text documents; They are from several genres, such as: News, Interviews, Blogs, Publicity
Texts, Web Pages, etc.;
focus: business documents such as contracts, CVs, personal data forms, and other documents present in the companies’
documentary databases
content: distinction between sensitive and personal data; GDPR; categories and types of sensitive data (Table 3.1);
NER framework: combination of rule-based models (p.35-41) + Lexicon-Based Models (p.41-43) + Machine Learning Models (p.43-52)
goal: the Recognition and Classification of sensitive data,
taking into account the group of classes of sensitive and personal data
---
(B3)
title: Investigating the Impact of Pre-trained Word Embeddings on Memorization in Neural Networks
author(s): Thomas et al.
year: 2020
abstract: The sensitive information present in the training data, poses
a privacy concern for applications as their unintended memorization during
training can make models susceptible to membership inference and
attribute inference attacks. In this paper, we investigate this problem in
various pre-trained word embeddings (GloVe, ELMo and BERT) with
the help of language models built on top of it. In particular, firstly sequences
containing sensitive information like a single-word disease and
4-digit PIN are randomly inserted into the training data, then a language
model is trained using word vectors as input features, and memorization
is measured with a metric termed as exposure. The embedding dimension,
the number of training epochs, and the length of the secret information
were observed to aect memorization in pre-trained embeddings.
Finally, to address the problem, dierentially private language models
were trained to reduce the exposure of sensitive information.
bibtex:
@inproceedings{ThomasAleena2020ItIo,
year = {2020},
title = {Investigating the Impact of Pre-trained Word Embeddings on Memorization in Neural Networks},
copyright = {Springer Nature Switzerland AG 2020},
language = {eng},
address = {Cham},
author = {Thomas, Aleena and Adelani, David Ifeoluwa and Davody, Ali and Mogadala, Aditya and Klakow, Dietrich},
keywords = {Differential privacy ; Unintended memorization ; Word representations},
series = {Lecture Notes in Computer Science},
issn = {0302-9743},
abstract = {The sensitive information present in the training data, poses a privacy concern for applications as their unintended memorization during training can make models susceptible to membership inference and attribute inference attacks. In this paper, we investigate this problem in various pre-trained word embeddings (GloVe, ELMo and BERT) with the help of language models built on top of it. In particular, firstly sequences containing sensitive information like a single-word disease and 4-digit PIN are randomly inserted into the training data, then a language model is trained using word vectors as input features, and memorization is measured with a metric termed as exposure. The embedding dimension, the number of training epochs, and the length of the secret information were observed to affect memorization in pre-trained embeddings. Finally, to address the problem, differentially private language models were trained to reduce the exposure of sensitive information.},
pages = {273--281},
volume = {12284},
publisher = {Springer International Publishing},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
isbn = {9783030583224},
}
keywords: differential privacy, word representations, unintended memorization
- research has shown recovery of sensitive information: memorization of training data or membership inference attack  
- leakage of publicly available models (Twitter-Glove, Clinical Bert) has not been quantified
- method: simple LM with word vectors as input
- exposure metric: to measure unintended memorization in neural networks; it uses a sorted list of log-perplexities of s[^r] (secrets);
Perplexity is a metric used to judge how good a language model is (see: https://towardsdatascience.com/perplexity-in-language-models-87a196019a94).
Is a model surprised by certain input (is it perplex)?
- Two types of secrets to study how the length of the secret aects its memorization: single-word-disease and four-digit-PIN;
- Two types of insertions to test the effect on the model by multiple secrets in the dataset: single insertion (either disease or PIN) and multiple insertions
- result: differential privacy helps reducing memorization, but slows down training process.
---
(B4)
title: Membership inference attacks against machine learning models
author(s): Shokri et al
year: 2017
bibtex:
@inproceedings{ShokriReza2017MIAA,
year = {2017},
title = {Membership Inference Attacks Against Machine Learning Models},
copyright = {Copyright 2017 Elsevier B.V., All rights reserved.},
language = {eng},
author = {Shokri, Reza and Stronati, Marco and Congzheng Song and Shmatikov, Vitaly},
keywords = {Data models ; Google ; Predictive models ; Privacy ; Sociology ; Statistics ; Training},
issn = {1081-6011},
abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
pages = {3--18},
publisher = {IEEE},
booktitle = {2017 IEEE Symposium on Security and Privacy (SP)},
isbn = {9781509055326},
}
content:
- p. 1: definition of membership inference attack
- p.1 : shadow training: knowledge of training data
- p.2: Machine learning as a service
- p.3: definition of privacy in machine learning
- black box scenario where the adversary can only supply inputs to the model and receive the model's ouput(s)
- output: prediction vector of probabilities
- inference attack setting: The attacker is given a data record and black-box query access to the target model;
- standard metric for attack accuracy: precision and recall
- attack model is trained to recognize differences in shadow models' behavior when these models operate on inputs from their own training datasets;
- p.6: Fig. 3 Training of attack model
- p.7: target models: two models constructed bycloud-based ML as a service plattform, one model implemented locally;
- p.8: experiment setup
- p.9: accuracy of attack: we use sets of the same size (equal number of members and non-members); in order to maximize the uncertainty of inference, thus the baseline accuracy is 0.5;
- p.10: results
a) models with fewer classes leak less information;
b) the more data in the training dataset is associated with a given class, the lower the attack precision for that class;
- p.12: results
There is an observable difference between the model output on the member imputs vs. the non-member inputs;
- p.12: methods against memorization of training data
e.g.: regularization techniques
---
(B5)
title: Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?
author(s): Hisamoto et al
year: 2020
bibtex:
@article{HisamotoSorami2020MIAo,
year = {2020},
title = {Membership Inference Attacks on Sequence-to-Sequence Models: Is My Data In Your Machine Translation System?},
copyright = {Copyright 2021 Elsevier B.V., All rights reserved.},
language = {eng},
address = {One Rogers Street, Cambridge, MA 02142-1209, USA},
author = {Hisamoto, Sorami and Post, Matt and Duh, Kevin},
keywords = {Inference ; Machine learning ; Machine translation},
issn = {2307-387X},
abstract = {Data privacy is an important issue for “machine learning as a
service” providers. We focus on the problem of membership inference
attacks: Given a data sample and black-box access to a model’s API,
determine whether the sample existed in the model’s training data. Our
contribution is an investigation of this problem in the context of
sequence-to-sequence models, which are important in applications such as machine
translation and video captioning. We define the membership inference problem for
sequence generation, provide an open dataset based on state-of-the-art machine
translation models, and report initial results on whether these models leak
private information against several kinds of membership inference attacks.},
journal = {Transactions of the Association for Computational Linguistics},
pages = {49--63},
volume = {8},
publisher = {MIT Press},
}
content:
+ focus: membership inference attack on sequence-generating models (machine translation)
+ introduction of characters like in security research literature
+ p. 54 attack accuracy: If the accuracy is 50%, then the binary
classification is same as random, and Alice is
safe. An accuracy slightly above 50% can be
considered a potential breach of privacy.
+ approach: shadow model attack
+ p.58-59: attack using groups of probes

---
(B6)
title: Exsense: Extract sensitive information from unstructured data
year: 2021
bibtex:
@article{GuoYongyan2021EEsi,
year = {2021},
title = {Exsense: Extract sensitive information from unstructured data},
copyright = {2020 Elsevier Ltd},
language = {eng},
address = {Amsterdam},
author = {Guo, Yongyan and Liu, Jiayong and Tang, Wenwu and Huang, Cheng},
keywords = {Algorithms ; Analysis ; Attention mechanism ; BERT ; Computational linguistics ; Computer security ; Content analysis ; Context ; Language processing ; Leakage ; Natural language interfaces ; Natural language processing ; Sensitive information ; Sequence labeling ; Unstructured data},
issn = {0167-4048},
abstract = {Large-scale sensitive information leakage incidents are frequently reported in recent years. Once sensitive information is leaked, it may lead to serious effects. In this context, sensitive information leakage has long been a question of great interest in the field of cybersecurity. However, most sensitive information resides in unstructured data. Therefore, how to extract sensitive information from voluminous unstructured data has become one of the greatest challenges. To address the above challenges, we propose a method named ExSense for extracting sensitive information from unstructured data, which utilizes the content-based and context-based extract mechanism. On the one hand, the method uses regular matching to extract sensitive information with predictable patterns. On the other hand, we build a model named BERT-BiLSTM-Attention for extracting sensitive information with natural language processing. This model uses the latest BERT algorithm to accomplish word embedding and extracts sensitive information by using BiLSTM and attention mechanism, with an F1 score of 99.15%. Experimental results on real-world datasets show that ExSense has a higher detection rate than using individual methods (i.e., content analysis and context analysis). In addition, we analyze about a million texts on Pastebin, and the results prove that ExSense can extract sensitive information from unstructured data effectively.},
journal = {Computers & security},
pages = {102156},
volume = {102},
publisher = {Elsevier Ltd},
}
content:
+ p.1-2: general info on information leakage (internal vs. external);
+ description of methods for data leakage prevention or sensitive information detection methods (content-based vs. context-based);
---

###(C) Information leakage in LMs###
(C1)
title: The Secret Sharer: Evaluating and Testing
Unintended Memorization in Neural Networks
author(s): Carlini et al.
year: 2019
abstract: This paper describes a testing methodology for quantitatively
assessing the risk that rare or unique training-data
sequences are unintentionally memorized by generative sequence
models—a common type of machine-learning model.
Because such models are sometimes trained on sensitive data
(e.g., the text of users’ private messages), this methodology
can benefit privacy by allowing deep-learning practitioners to
select means of training that minimize such memorization.
In experiments, we show that unintended memorization is
a persistent, hard-to-avoid issue that can have serious consequences.
Specifically, for models trained without consideration
of memorization, we describe new, efficient procedures
that can extract unique, secret sequences, such as credit card
numbers. We show that our testing strategy is a practical and
easy-to-use first line of defense, e.g., by describing its application
to quantitatively limit data exposure in Google’s
Smart Compose, a commercial text-completion neural network
trained on millions of users’ email messages.
bibtex:
@article{CarliniNicholas2018TSSE,
year = {2018},
title = {The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
abstract = {This paper describes a testing methodology for quantitatively assessing the
risk that rare or unique training-data sequences are unintentionally memorized
by generative sequence models---a common type of machine-learning model.
Because such models are sometimes trained on sensitive data (e.g., the text of
users' private messages), this methodology can benefit privacy by allowing
deep-learning practitioners to select means of training that minimize such
memorization.
In experiments, we show that unintended memorization is a persistent,
hard-to-avoid issue that can have serious consequences. Specifically, for
models trained without consideration of memorization, we describe new,
efficient procedures that can extract unique, secret sequences, such as credit
card numbers. We show that our testing strategy is a practical and easy-to-use
first line of defense, e.g., by describing its application to quantitatively
limit data exposure in Google's Smart Compose, a commercial text-completion
neural network trained on millions of users' email messages.},
}
content:
Testing methodology: threat model using targeted, probing queries
Testing procedure:
- insert randomly chosen "canary" sequences into training data
- exposure metric to measure the relative difference in perplexity between canaries and non-inserted sequences
term: perplexity = natural likelihood measure
The log perplexity of a sequence measures how well the LM predicts the tokens in that sequences.
If the perplexity is low, then the model is not "surprised" by the sequence and has assigned 
on average a high probability to each subsequent token in the sequence. 
-guessing entropy: number of guesses E(x) required to guess the value of a discrete random variable x
- p. 273 unintended memorization: memorization of out-of-distribution values (not in-distribution values helpulf for learning the task. 
- difference to thesis: train model(s) with canaries, focus on rare training instances ("secrets")
- best practices for processing sensitive data:
a) sanitization (e.g. black lists to filter out sentences with sentitve info)
b) differential privacy
- exposure can be seen as an improvement that quantifies how much memorization has occurred (and not just if --> cf. membership inference attack)
---
(C2)
title: Extracting Training Data from Large Language Models
author(s): Carlini et al.
year: 2020
abstract: It has become common to publish large (billion parameter)
language models that have been trained on private datasets.
This paper demonstrates that in such settings, an adversary can
perform a training data extraction attack to recover individual
training examples by querying the language model.
We demonstrate our attack on GPT-2, a language model
trained on scrapes of the public Internet, and are able to extract
hundreds of verbatim text sequences from the model’s training
data. These extracted examples include (public) personally
identifiable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our
attack is possible even though each of the above sequences
are included in just one document in the training data.
We comprehensively evaluate our extraction attack to understand
the factors that contribute to its success. Worryingly,
we find that larger models are more vulnerable than smaller
models. We conclude by drawing lessons and discussing possible
safeguards for training large language models.
bibtex:
@article{CarliniNicholas2020ETDf,
year = {2020},
title = {Extracting Training Data from Large Language Models},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
abstract = {It has become common to publish large (billion parameter) language models
that have been trained on private datasets. This paper demonstrates that in
such settings, an adversary can perform a training data extraction attack to
recover individual training examples by querying the language model.
We demonstrate our attack on GPT-2, a language model trained on scrapes of
the public Internet, and are able to extract hundreds of verbatim text
sequences from the model's training data. These extracted examples include
(public) personally identifiable information (names, phone numbers, and email
addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible
even though each of the above sequences are included in just one document in
the training data.
We comprehensively evaluate our extraction attack to understand the factors
that contribute to its success. Worryingly, we find that larger models are more
vulnerable than smaller models. We conclude by drawing lessons and discussing
possible safeguards for training large language models.}
content:
Testing methodology: adversary with black-box input-output access to a LM model; indiscriminately extract training data (not targeted)
Testing procedure:
- generate large quantity of data
- predict which outputs may contain memorized text using membership inference attack. Past membership inference attacks
rely on the observation that models tend to assign higher confidence to examples that are present in the training data
- internet-search to manually mark the text samples as memorized/not memorized
- privacy leakage of models typically associated with overfitting
- p. 2634-35: good overview/description of LLMs
- p 2635-6: definition of model memorization (eidetic memorization)
- adversary with black-box input-output access to LM
- difference to thesis: indiscriminately extract training data, NOT targeted pieces of training data;
- framework of data privacy as contextual integrity (vgl. GDPR)
- p. 2638: formula for perplexity
- manual inspection of samples which are suspects for memorization by manual internet searches for exact string matches
- p.2642: GPT2 memorizes content that has been deleted from the internet (violates right to be forgotten) --> LMs may serve as unintentional archives for removed data.
- p.2643: importance of context; more descriptive prompt (pi is 3.1...)
- p. 2644: LM size and memorization: larger models memorize significantly more training data;
- p. 2645: future work: how memorization is inherited by fine-tuned models
---
(C3)
title: What does GPT-3 “know” about me? 
author(s): Melissa Heikkilä
year: 2022
url: https://www.technologyreview.com/2022/08/31/1058800/what-does-gpt-3-know-about-me/?utm_medium=tr_social&utm_campaign=site_visitor.unpaid.engagement&utm_source=Twitter
content:
LLMs such as OpenAI’s GPT-3, Google’s LaMDA, and Meta’s OPT-175B, used to power chatbots;
Google and OpenAI do not release information about the data sets that have been used to build their language models;
efforts to improve the privacy of machine learning and regulate the technology are still in their infancy;
most large language models are very US-focused. The US does not have a federal data protection law;
Occasionally, the model may generate information that is not factually accurate because it is attempting to produce
plausible text based on statistical patterns in its training data and context provided by the user;
fairly easy for hackers to actively tamper with a data set by “poisoning” it with data of their choosing in order to create insecurities that allow for security breaches;
It’s not just personal data. The data sets are likely to include data that is copyrighted, such as source code and books;
Private data is often scattered throughout the data sets used to train LLMs, many of which are scraped off the open internet;
The more often those personal bits of information appear in the training data, the more likely the model is to memorize them, and the stronger the association becomes;
---
(C4)
title: Are Large Pre-Trained Language Models Leaking your Personal Information?
author(s): Huang et al.
year: 2022
bibtex:
@article{HuangJie2022ALPL,
year = {2022},
title = {Are Large Pre-Trained Language Models Leaking Your Personal Information?},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Huang, Jie and Shao, Hanyin and Chang, Kevin Chen-Chuan},
abstract = {Are Large Pre-Trained Language Models Leaking Your Personal Information? In
this paper, we analyze whether Pre-Trained Language Models (PLMs) are prone to
leaking personal information. Specifically, we query PLMs for email addresses
with contexts of the email address or prompts containing the owner's name. We
find that PLMs do leak personal information due to memorization. However, since
the models are weak at association, the risk of specific personal information
being extracted by attackers is low. We hope this work could help the community
to better understand the privacy risk of PLMs and bring new insights to make
PLMs safe.},
}
content:
- two capacities that may cause privacy leakage:
a) memorization:information can be recovered with a specific prefix (using greedy search), e.g. my security number is ...
b) association: query attack with owner's name (using greedy search), e.g. the email-address of Tom is ...
- focus of research: recovery of email-addresses
- task: measure the risk of PLMs in terms of leaking personal information
- attack: 
a) given the context, examine whether the model can recover the email address
b) given the name of the owner, query the model for the associated email address with an appropriate prompt.
- 3238 (name, email) pairs for experiments
- results:
+ low accuracy for context (Table 2) and domain unknown (Table 3) approach
+ huge improvement in prediction accuracy when domain is known (Table 4)
+ PLMs do memorize email-addresses, but do not understand the exact associations between names and email-addresses
+ PLMs make predictions based on the memorization of sequences --> longer contexts can discover more memorization
+ PLMs with more parameters are able to memorize more trianing data;
+ Some conditions increase the attack success rate: long text patterns, knowledge about the owner, scale of the model
---
(C5)
title: Training Data Leakage Analysis in Language Models
author(s): Inan et al.
year: 2021
bibtex:
@article{InanHuseyinA2021TDLA,
year = {2021},
title = {Training Data Leakage Analysis in Language Models},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Inan, Huseyin A and Ramadan, Osman and Wutschitz, Lukas and Jones, Daniel and Rühle, Victor and Withers, James and Sim, Robert},
abstract = {Recent advances in neural network based language models lead to successful
deployments of such models, improving user experience in various applications.
It has been demonstrated that strong performance of language models comes along
with the ability to memorize rare training samples, which poses serious privacy
threats in case the model is trained on confidential user content. In this
work, we introduce a methodology that investigates identifying the user content
in the training data that could be leaked under a strong and realistic threat
model. We propose two metrics to quantify user-level data leakage by measuring
a model's ability to produce unique sentence fragments within training data.
Our metrics further enable comparing different models trained on the same data
in terms of privacy. We demonstrate our approach through extensive numerical
studies on both RNN and Transformer based models. We further illustrate how the
proposed metrics can be utilized to investigate the efficacy of mitigations
like differentially private training or API hardening.},
content:
- quality of a LM is commonly measured by perplexity (likelihood of text sequences) and top-k accuracy (ratio of the number of correct predictions to the total number of tokens). 
- tab attack: the unique sequences that could be leaked via the tab attack when then model is queried with the corresponding context
- attack on transfomer-based model: GPT-2
- differentiation between public LMs (trained with public dataset) and private LMs (trained with privat dataset)
- attacks rely on the model output beyond top-1 or top-3 predictions along with the perplexity measure

---
(C6)
title:When Machine Learning Models Leak: An Exploration of Synthetic Training Data
author(s): Slokom et al.
year:2022
abstract: We investigate an attack on a machine learning classifier that
predicts the propensity of a person or household to move (i.e., relocate)
in the next two years. The attack assumes that the classifier has been
made publically available and that the attacker has access to information
about a certain number of target individuals. That attacker might
also have information about another set of people to train an auxiliary
classifier. We show that the attack is possible for target individuals
independently of whether they were contained in the original training
set of the classifier. However, the attack is somewhat less successful for
individuals that were not contained in the original data. Based on this
observation, we investigate whether training the classifier on a data set
that is synthesized from the original training data, rather than using the
original training data directly, would help to mitigate the effectiveness
of the attack. Our experimental results show that it does not, leading us
to conclude that new approaches to data synthesis must be developed if
synthesized data is to resemble “unseen” individuals to an extent great
enough to help to block machine learning model attacks.
bibtex:
@inproceedings{10.1007/978-3-031-13945-1_20,
author = {Slokom, Manel and de Wolf, Peter-Paul and Larson, Martha},
title = {When Machine Learning Models Leak: An Exploration of&nbsp;Synthetic Training Data},
year = {2022},
isbn = {978-3-031-13944-4},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-13945-1_20},
doi = {10.1007/978-3-031-13945-1_20},
abstract = {We investigate an attack on a machine learning classifier that predicts the propensity of a person or household to move (i.e., relocate) in the next two years. The attack assumes that the classifier has been made publically available and that the attacker has access to information about a certain number of target individuals. That attacker might also have information about another set of people to train an auxiliary classifier. We show that the attack is possible for target individuals independently of whether they were contained in the original training set of the classifier. However, the attack is somewhat less successful for individuals that were not contained in the original data. Based on this observation, we investigate whether training the classifier on a data set that is synthesized from the original training data, rather than using the original training data directly, would help to mitigate the effectiveness of the attack. Our experimental results show that it does not, leading us to conclude that new approaches to data synthesis must be developed if synthesized data is to resemble “unseen” individuals to an extent great enough to help to block machine learning model attacks.},
booktitle = {Privacy in Statistical Databases: International Conference, PSD 2022, Paris, France, September 21–23, 2022, Proceedings},
pages = {283–296},
numpages = {14},
keywords = {Synthetic data, Propensity to move, Attribute inference, Machine learning},
location = {Paris, France}
}
content:
- goal: to test whether a machine learning model trained on synthetic data can replace a machine learning model trained on original data.
- p. 286: privacy in ML: can be divided into three categories:
a) making the ML model private
b) using ML to enhance privacy protection
c) ML-based privacy attack
- p.287 synthetic data: captures the distribution of the original (training) data and generates artifical, but realistic data;
- p.287 Statistical Disclosure Control(SDC) distinguishes two types of synthetic data:
a) fully synthetic data sets
b) partially synthetic data sets
- p.288 difference black box/white box attack: amount of resources that are available for the adversary;
- p.290:
a) Random attack : uses a subset of data and marginal prior distribution.
b) Baseline attack: uses a subset of data, marginal prior distribution, and random forest classifier.
c) Black-box attack: uses a subset of data, marginal prior distribution, released ML model, and random forest classifier.
- p.291:
a) precision: measures the ability not to label a negative sample as positive; tp/(tp+fp)
b) recall. measures the ability to find all the positive samples; tp/(tp+fn)
---
(C7)
title: Quantifying Memorization Across Neural Language Models
author(s): Carlini et al.
year: 2022
bibtex:
@article{CarliniNicholas2022QMAN,
year = {2022},
title = {Quantifying Memorization Across Neural Language Models},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
abstract = {Large language models (LMs) have been shown to memorize parts of their
training data, and when prompted appropriately, they will emit the memorized
training data verbatim. This is undesirable because memorization violates
privacy (exposing user data), degrades utility (repeated easy-to-memorize text
is often low quality), and hurts fairness (some texts are memorized over
others).
We describe three log-linear relationships that quantify the degree to which
LMs emit memorized training data. Memorization significantly grows as we
increase (1) the capacity of a model, (2) the number of times an example has
been duplicated, and (3) the number of tokens of context used to prompt the
model. Surprisingly, we find the situation becomes complicated when
generalizing these results across model families. On the whole, we find that
memorization in LMs is more prevalent than previously believed and will likely
get worse as models continues to scale, at least without active mitigations.},
}
content:
+ p.2 approach: we feed varying-length prefixes of the training data back into the trained model, and verify whether the model has the ability to complete the rest of the example verbatim;
+ three properties that impact memorization significantly:
a) model scale p.5
b) data duplication p.5
c) context p.6
---
(C8)
title: ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning
author(s): Murakonda and Shokri
year: 2020
bibtex:
@article{MurakondaSasiKumar2020MPMA,
year = {2020},
title = {ML Privacy Meter: Aiding Regulatory Compliance by Quantifying the Privacy Risks of Machine Learning},
copyright = {http://creativecommons.org/licenses/by/4.0},
language = {eng},
author = {Murakonda, Sasi Kumar and Shokri, Reza},
abstract = {When building machine learning models using sensitive data, organizations
should ensure that the data processed in such systems is adequately protected.
For projects involving machine learning on personal data, Article 35 of the
GDPR mandates it to perform a Data Protection Impact Assessment (DPIA). In
addition to the threats of illegitimate access to data through security
breaches, machine learning models pose an additional privacy risk to the data
by indirectly revealing about it through the model predictions and parameters.
Guidances released by the Information Commissioner's Office (UK) and the
National Institute of Standards and Technology (US) emphasize on the threat to
data from models and recommend organizations to account for and estimate these
risks to comply with data protection regulations. Hence, there is an immediate
need for a tool that can quantify the privacy risk to data from models.
In this paper, we focus on this indirect leakage about training data from
machine learning models. We present ML Privacy Meter, a tool that can quantify
the privacy risk to data from models through state of the art membership
inference attack techniques. We discuss how this tool can help practitioners in
compliance with data protection regulations, when deploying machine learning
models.},
}
content:
+ p.1: Data
protection regulations, such as GDPR, and AI governance frameworks require personal data to be protected when used
in AI systems, and that the users have control over their data and awareness about how it is being used.
+ p.2: We present ML Privacy Meter that can quantify the privacy risks to training data and is based on well-established algo-
rithms to measure privacy risks of machine learning models through membership inference attacks. ML Privacy Meter works by implementing membership
inference attacks against machine learning models. It simulates attackers with different levels of access and knowledge about the model.
---
(C9)
title: A Survey of Privacy Attacks in Machine Learning
auhtor(s): Rigaki and Garcia
year: 2020
abstract: As machine learning becomes more widely used, the need to study its implications in security and privacy
becomes more urgent. Although the body of work in privacy has been steadily growing over the past few years,
research on the privacy aspects of machine learning has received less focus than the security aspects. Our
contribution in this research is an analysis of more than 40 papers related to privacy attacks against machine
learning that have been published during the past seven years. We propose an attack taxonomy, together with
a threat model that allows the categorization of different attacks based on the adversarial knowledge, and the
assets under attack. An initial exploration of the causes of privacy leaks is presented, as well as a detailed
analysis of the different attacks. Finally, we present an overview of the most commonly proposed defenses
and a discussion of the open problems and future directions identified during our analysis.
bibtex:
@article{DBLP:journals/corr/abs-2007-07646,
  author    = {Maria Rigaki and
               Sebastian Garcia},
  title     = {A Survey of Privacy Attacks in Machine Learning},
  journal   = {CoRR},
  volume    = {abs/2007.07646},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.07646},
  eprinttype = {arXiv},
  eprint    = {2007.07646},
  timestamp = {Tue, 21 Jul 2020 12:53:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-07646.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
content:
+ research on the privacy aspects of machine learning has received less focus than the security aspects.
+ the term "adversarial attacks" is commonly used to refer to security-related attacks
+ provides taxonomy for privacy attacks
+ p.2: models could be classified as personal data under the GDPR
+ p.8-10: attack types
a) membership inference
b) reconstruction
c) property inference
d) model extraction
+ p.10: causes of privacy leaks
+ causes for membership inference:
a) overfitting / generalization error
b) model complexity
c) model type and number of classes
+ p.12: explanation of shadow models and shadow training;
+ p.13: One of the main assumptions of membership inference attacks on supervised learning models is that the adversary has no or limited knowledge
of the training samples used. However, the adversary knows something about the underlying data distribution of the training data.
+ p.14: The initial reconstruction attacks were based on the assumption that
the adversary has access to the model 𝑓 , the priors of the sensitive and nonsensitive features, and the
output of the model for a specific input 𝑥.
+ p.15: In property inference the shadow datasets are labeled based on
the properties that the adversary wants to infer, so the adversary needs access to data that have
the property and data that do not have it.
+ p.18: The most popular attack types are membership inference and reconstruction attacks (35.7% of
the papers, respectively); The majority of the proposed attacks are performed during the inference phase (88%). The focus on neural networks in the existing literature as well as the focus on supervised learning
is also apparent in Figure 4.
+ p.20: Figure5 As the figure clearly shows, the majority of the attacks are on models
that were trained for classification tasks, both binary and multiclass. This is the case across all four attack types.
---
(C10)
title: Are Your Sensitive Attributes Private?
author(s): Mehnaz et al.
year: 2022
abstract: Increasing use of machine learning (ML) technologies in
privacy-sensitive domains such as medical diagnoses, lifestyle
predictions, and business decisions highlights the need to better understand if these ML technologies are introducing leakage of sensitive and proprietary training data. In this paper, we
focus on model inversion attacks where the adversary knows
non-sensitive attributes about records in the training data and
aims to infer the value of a sensitive attribute unknown to the
adversary, using only black-box access to the target classification model. We first devise a novel confidence score-based
model inversion attribute inference attack that significantly
outperforms the state-of-the-art. We then introduce a labelonly model inversion attack that relies only on the model’s
predicted labels but still matches our confidence score-based
attack in terms of attack effectiveness. We also extend our
attacks to the scenario where some of the other (non-sensitive)
attributes of a target record are unknown to the adversary. We
evaluate our attacks on two types of machine learning models,
decision tree and deep neural network, trained on three real
datasets. Moreover, we empirically demonstrate the disparate
vulnerability of model inversion attacks, i.e., specific groups
in the training dataset (grouped by gender, race, etc.) could
be more vulnerable to model inversion attacks.
bibtex:
@inproceedings {280036,
author = {Shagufta Mehnaz and Sayanton V. Dibbo and Ehsanul Kabir and Ninghui Li and Elisa Bertino},
title = {Are Your Sensitive Attributes Private? Novel Model Inversion Attribute Inference Attacks on Classification Models},
booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
year = {2022},
isbn = {978-1-939133-31-1},
address = {Boston, MA},
pages = {4579--4596},
url = {https://www.usenix.org/conference/usenixsecurity22/presentation/mehnaz},
publisher = {USENIX Association},
month = aug,
}
content:
+ focus on reconstruction attacks (model attribute inference and typical instance reconstruction)
---
(C11)
title: On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?
author(s): Bender et al.
year: 2021
abstract: The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.
biibtex:
@inproceedings{10.1145/3442188.3445922,
author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.uaccess.univie.ac.at/10.1145/3442188.3445922},
doi = {10.1145/3442188.3445922},
abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {610–623},
numpages = {14},
location = {Virtual Event, Canada},
series = {FAccT '21}
}
content:
+ focus: potential risks of very large LLMs (number of parameters and size of training data)
+ p.610: difficulty of understanding what is in the training data scales with model size; 
+ p.611: definition of LM --> which are trained on string prediction tasks: that is, predicting the likelihood of a token (character, word or string) given
either its preceding context or (in bidirectional and masked LMs) its surrounding context.
+ this trend of increasingly large LMs can be expected to continue as long as they correlate with an increase in performance.
---
(C12)
title: Comprehensive Privacy Analysis of Deep Learning
year: 2019
bibtex:
@inproceedings{NasrMilad2019CPAo,
year = {2019},
title = {Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning},
copyright = {Copyright 2019 Elsevier B.V., All rights reserved.},
language = {eng},
address = {Ithaca},
author = {Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
keywords = {Algorithms ; Artificial neural networks ; Computational modeling ; Data models ; Deep learning ; Federated-Learning ; Inference ; Inference algorithms ; Inference-Attacks ; Machine learning ; Mathematical models ; Membership-Inference ; Model accuracy ; Neural networks ; Parameters ; Privacy ; Stochastic-Gradient-Descent ; Training ; Training data},
issn = {1081-6011},
abstract = {Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.},
pages = {739--753},
volume = {2019-},
publisher = {IEEE},
booktitle = {2019 IEEE Symposium on Security and Privacy (SP)},
isbn = {9781538666609},
}
content:
+ p.739: central question: What is the privacy risk of deep learning
algorithms to individuals whose data is used for training deep
neural networks?
+ definition of privacy sensitive leakage 
+ definition of inference attacks (membership inference vs. recosntruction)
+ p.740: catgeorization of attacks (see Table 1)
+ attack observations: black-box vs. white-box
+ p.742: exploit the algorithm used to train the deep learning models (SGD), not only the ouput or intermediate computations (hidden layers);
+ claim: the first layers tend to contain less info about the specific data points in the training set;
+ p.743: inference target --> stand alone fine-tuned model
attack mode --> active vs. passive 
+ p.744: active attack mode: If the target record x is in the training set of a participant,
its local SGD algorithm abruptly reduces the gradient of the
loss on x. This can be detected by the inference model, and
be used to distinguish members from non-members.
+ p.745: evaluation metrics for attack model (trained)
a) attack accuracy
b) True/False positives
c) prediction uncertainty
+ p.746: result of stand-alone, white-box attack: among the layer outputs, the last layer
(model output) leaks the most membership information about the training data; BUT gradients leak significantly more membership information
about the training set, compared to the layer outputs.
+ p.751: discussion o related work (membership inference attacks, other inferernce attacks etc.)
---
(C13)
title: Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?
year: 2021
bibtex:
@article{LehmanEric2021DBPo,
year = {2021},
title = {Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Lehman, Eric and Jain, Sarthak and Pichotta, Karl and Goldberg, Yoav and Wallace, Byron C},
abstract = {Large Transformers pretrained over clinical notes from Electronic Health
Records (EHR) have afforded substantial gains in performance on predictive
clinical tasks. The cost of training such models (and the necessity of data
access to do so) coupled with their utility motivates parameter sharing, i.e.,
the release of pretrained models such as ClinicalBERT. While most efforts have
used deidentified EHR, many researchers have access to large sets of sensitive,
non-deidentified EHR with which they might train a BERT model (or similar).
Would it be safe to release the weights of such a model if they did? In this
work, we design a battery of approaches intended to recover Personal Health
Information (PHI) from a trained BERT. Specifically, we attempt to recover
patient names and conditions with which they are associated. We find that
simple probing methods are not able to meaningfully extract sensitive
information from BERT trained over the MIMIC-III corpus of EHR. However, more
sophisticated "attacks" may succeed in doing so: To facilitate such research,
we make our experimental setup and baseline probing models available at
https://github.com/elehman16/exposing_patient_data_release},
}
content:
+ p.948: focus: quantifying the risks of releasing contextualized embedding weights trained on non-deidentified text;
+ aim: recovering name-condition pairs
+ p. 952: attempt to determine whether a pretrained model has seen a particular patient name in training; approach --> probing 
---
(C14)
title: MemorizationWithout Overfitting: Analyzing the Training Dynamics of Large Language Models
year: 2022
bibtex:
@article{TirumalaKushal2022MWOA,
year = {2022},
title = {Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Tirumala, Kushal and Markosyan, Aram H and Zettlemoyer, Luke and Aghajanyan, Armen},
keywords = {Computer Science - Computation and Language},
abstract = {Despite their wide adoption, the underlying training and memorization
dynamics of very large language models is not well understood. We empirically
study exact memorization in causal and masked language modeling, across model
sizes and throughout the training process. We measure the effects of dataset
size, learning rate, and model size on memorization, finding that larger
language models memorize training data faster across all settings.
Surprisingly, we show that larger models can memorize a larger portion of the
data before over-fitting and tend to forget less throughout the training
process. We also analyze the memorization dynamics of different parts of speech
and find that models memorize nouns and numbers first; we hypothesize and
provide empirical evidence that nouns and numbers act as a unique identifier
for memorizing individual training examples. Together, these findings present
another piece of the broader puzzle of trying to understand what actually
improves as models get bigger.},
}
content:
+ p.1: Larger models are also known to memorize more training data [8], which is a crucial component of their improved generalization.
+ p.5: Typically, memorization is associated with overfitting, which offers a potentially simple explanation. BUT overfitting by itself cannot completely explain
the properties of memorization dynamics as model scale increases.
+ p.7: Specifically, nouns, proper nouns, and numerals
are memorized noticeably faster than verbs and adjectives, both in terms of R(p) and Rmem(p). This
has implications for privacy since sensitive information is likely to be a noun/proper noun/numeral.
---
(C14-2)
title: To Trust or Not To Trust Prediction Scores for Membership Inference Attacks
year: 2021
bibtex: @article{HintersdorfDominik2021TToN,
year = {2021},
title = {To Trust or Not To Trust Prediction Scores for Membership Inference Attacks},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Hintersdorf, Dominik and Struppek, Lukas and Kersting, Kristian},
abstract = {Membership inference attacks (MIAs) aim to determine whether a specific
sample was used to train a predictive model. Knowing this may indeed lead to a
privacy breach. Most MIAs, however, make use of the model's prediction scores -
the probability of each output given some input - following the intuition that
the trained model tends to behave differently on its training data. We argue
that this is a fallacy for many modern deep network architectures.
Consequently, MIAs will miserably fail since overconfidence leads to high
false-positive rates not only on known domains but also on out-of-distribution
data and implicitly acts as a defense against MIAs. Specifically, using
generative adversarial networks, we are able to produce a potentially infinite
number of samples falsely classified as part of the training data. In other
words, the threat of MIAs is overestimated, and less information is leaked than
previously assumed. Moreover, there is actually a trade-off between the
overconfidence of models and their susceptibility to MIAs: the more classifiers
know when they do not know, making low confidence predictions, the more they
reveal the training data.},
}
content:
+ p.1: to prove that a specific data point was part of the training set is difficult since neural networks
do not store plain training data like lazy learners. Instead, the learned knowledge is encoded into the network’s weights.
+ p.2: there might not exist any meaningful MIA at all since the attacks will always produce
a high number of false positives due to the overconfidence of neural networks.
+ p.2 description of general membership inference attack: an adversary is given an input x following distribution D and
a target model Mtarget which was trained on a training set Starget train Dn with size n. The adversary is then facing the problem to identify whether a given x D was part of the training set Starget train.
To predict the membership of x, the adversary creates an inference model h. In score-based MIAs, the input to h is the prediction score vector produced by Mtarget on sample x.
All MIAs exploit a difference in the behavior of Mtarget on seen and unseen data. Most attacks in the literature follow Shokri et al. [25] and train so-called shadow models Mshadow on a disjoint dataset Sshadow
train drawn from the same distribution D as Starget train.
+ p.3: Neural networks usually output prediction scores, e.g., by applying a softmax function.
+ p.5: neural networks are overconfident in their predictions, even on inputs without any known content.
It prevents a reasonable interpretation regarding a model’s probability of being correct in its predictions. During
MIAs, on the other side, this behavior implicitly protects the training data since the information content of the prediction score is rather low.
+ p.7: result: Our experiments underline
the known fact that modern neural networks are not inherently able to identify unseen and unknown inputs and
cannot adapt their behavior in terms of reducing the prediction sscores. We have shown that MIAs produce high false-positive
rates due to overconfident predictions of modern neural networks for in- and out-of-distribution data. Overconfidence can be
seen as a natural defense against these attacks.
---
(C15)
title: Information Leakage in Embedding Models
year: 2020
bibtex:
@inproceedings{SongCongzheng2020ILiE,
year = {2020},
title = {Information Leakage in Embedding Models},
copyright = {Copyright 2020 Elsevier B.V., All rights reserved.},
language = {eng},
author = {Song, Congzheng and Raghunathan, Ananth},
keywords = {embeddings ; machine learning ; privacy},
issn = {1543-7221},
pages = {377--390},
booktitle = {Proceedings of the ACM Conference on Computer and Communications Security},
isbn = {1450370896},
}
content:
+ focus: embeddings, transfer learning:
+ p.2: Embedding models are often pre-trained on raw and unlabeled
data at hand, then used with labeled data to transfer learning to
various downstream tasks.
+ we consider attribute inference attacks to test if embeddings might unintentionally
reveal attributes of their input data that are sensitive; aspects that are not useful for their downstream applications.
+ p.10: evaluation of membership inference attack with adversarial advantage
---
(C16)
title: Can pre-trained Transformers be used in detecting complex sensitive sentences?
year: 2022 
bibtex:
@article{TimmerRoelienC2022CpTb,
year = {2022},
title = {Can pre-trained Transformers be used in detecting complex sensitive sentences? -- A Monsanto case study},
copyright = {http://creativecommons.org/licenses/by-nc-sa/4.0},
language = {eng},
author = {Timmer, Roelien C and Liebowitz, David and Nepal, Surya and Kanhere, Salil S},
keywords = {Computer Science - Computation and Language},
abstract = {Each and every organisation releases information in a variety of forms
ranging from annual reports to legal proceedings. Such documents may contain
sensitive information and releasing them openly may lead to the leakage of
confidential information. Detection of sentences that contain sensitive
information in documents can help organisations prevent the leakage of valuable
confidential information. This is especially challenging when such sentences
contain a substantial amount of information or are paraphrased versions of
known sensitive content. Current approaches to sensitive information detection
in such complex settings are based on keyword-based approaches or standard
machine learning models. In this paper, we wish to explore whether pre-trained
transformer models are well suited to detect complex sensitive information.
Pre-trained transformers are typically trained on an enormous amount of text
and therefore readily learn grammar, structure and other linguistic features,
making them particularly attractive for this task. Through our experiments on
the Monsanto trial data set, we observe that the fine-tuned Bidirectional
Encoder Representations from Transformers (BERT) transformer model performs
better than traditional models. We experimented with four different categories
of documents in the Monsanto dataset and observed that BERT achieves better F2
scores by 24.13\% to 65.79\% for GHOST, 30.14\% to 54.88\% for TOXIC, 39.22\%
for CHEMI, 53.57\% for REGUL compared to existing sensitive information
detection models.},
}
content:
+ p.1: aim: use BERT to detect complex sensitive information in text;
+ p.3: BERT is the most well-known transformer
+ p.5: results (Table III): For all the four sub data sets, our fine-tuned pre-trained BERT model has the same or a higher score for all the five different
metrics. This suggests that it is not only more accurate in prediction but also accurately detects sensitive information.

---
(C17)
title: Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures
year: 2015
bibtex:
@inproceedings{10.1145/2810103.2813677,
author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
title = {Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures},
year = {2015},
isbn = {9781450338325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi-org.uaccess.univie.ac.at/10.1145/2810103.2813677},
doi = {10.1145/2810103.2813677},
abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.},
booktitle = {Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
pages = {1322–1333},
numpages = {12},
keywords = {attacks, machine learning, privacy},
location = {Denver, Colorado, USA},
series = {CCS '15}
}
content:
+ p.1322: We develop a new class of model inversion attack that exploits confidence values revealed along with predictions.
+ distinction white-box black-box access
+ model inversion attack
+ p.1327: results: The largest difference between black-box and white-box accuracy is precision: the white-box attack gave no false positives, whereas black-box yielded about 15% in
FiveThirtyEight and about 60% on GSS.
---


###(D) Differential Privacy###
(D1)
title: The Algorithmic Foundations of Differential Privacy
auhtor(s): Dwork & Roth
year: 2014
abstract: The problem of privacy-preserving data analysis has a long history
spanning multiple disciplines. As electronic data about individuals
becomes increasingly detailed, and as technology enables ever more
powerful collection and curation of these data, the need increases for a
robust, meaningful, and mathematically rigorous definition of privacy,
together with a computationally rich class of algorithms that satisfy
this definition. Differential Privacy is such a definition.
After motivating and discussing the meaning of differential privacy,
the preponderance of this monograph is devoted to fundamental techniques
for achieving differential privacy, and application of these techniques
in creative combinations, using the query-release problem as an
ongoing example. A key point is that, by rethinking the computational
goal, one can often obtain far better results than would be achieved by
methodically replacing each step of a non-private computation with a
differentially private implementation. Despite some astonishingly powerful
computational results, there are still fundamental limitations —
not just on what can be achieved with differential privacy but on what
can be achieved with any method that protects against a complete
breakdown in privacy. Virtually all the algorithms discussed herein
maintain differential privacy against adversaries of arbitrary computational
power. Certain algorithms are computationally intensive, others
are efficient. Computational complexity for the adversary and the
algorithm are both discussed.
We then turn from fundamentals to applications other than queryrelease,
discussing differentially private methods for mechanism design
and machine learning. The vast majority of the literature on differentially
private algorithms considers a single, static, database that is subject
to many analyses. Differential privacy in other models, including
distributed databases and computations on data streams is discussed.
Finally, we note that this work is meant as a thorough introduction
to the problems and techniques of differential privacy, but is not
intended to be an exhaustive survey—there is by now a vast amount of
work in differential privacy, and we can cover only a small portion of it.
bibtex:
@article{TCS-042,
url = {http://dx.doi.org/10.1561/0400000042},
year = {2014},
volume = {9},
journal = {Foundations and Trends® in Theoretical Computer Science},
title = {The Algorithmic Foundations of Differential Privacy},
doi = {10.1561/0400000042},
issn = {1551-305X},
number = {3–4},
pages = {211-407},
author = {Cynthia Dwork and Aaron Roth}
}
content:
- Definition of differential privacy:
Differential privacy will provide privacy by process; in particular it will introduce randomness.
Differential privacy promises that the behavior of an algorithm will be roughly unchanged even if a single entry in
the database is modified.
Differential privacy promises to protect individuals
from any additional harm that they might face due to their data
being in the private database x that they would not have faced had
their data not been part of x.
- important concepts/terms:
+ query: a query is a function to be applied to a database
+ privacy mechanism: A privacy mechanism, or simply a mechanism, is an algorithm that
takes as input a database, a universe X of data types (the set of all
possible database rows), random bits, and, optionally, a set of queries,
and produces an output string
***read until chapter 3***
---
(D2)
title: Deep Learning with Differential Privacy
auhtor(s): Abadi et al.
year: 2016
abstract: Machine learning techniques based on neural networks are
achieving remarkable results in a wide variety of domains.
Often, the training of models requires large, representative
datasets, which may be crowdsourced and contain sensitive
information. The models should not expose private information
in these datasets. Addressing this goal, we develop new
algorithmic techniques for learning and a refined analysis of
privacy costs within the framework of dfiferential privacy.
Our implementation and experiments demonstrate that we
can train deep neural networks with non-convex objectives,
under a modest privacy budget, and at a manageable cost in
software complexity, training efficiency, and model quality.
bibtex:
@inproceedings{AbadiMartín2016DLwD,
year = {2016},
title = {Deep Learning with Differential Privacy},
copyright = {2016. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
language = {eng},
address = {Ithaca},
author = {Abadi, Martín and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
keywords = {Cost analysis ; Datasets ; Deep learning ; Domains ; Machine learning ; Neural networks ; Privacy ; Training},
issn = {2331-8422},
abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
publisher = {Cornell University Library, arXiv.org},
booktitle = {arXiv.org},
}
content:
- claim:
Differential privacy has several properties that make it
particularly useful in applications such as ours: composability,
group privacy, and robustness to auxiliary information.
- approach:
+ They aim to control the influence of the training data during
the training process, specificcally in the SGD computation.
+ They compute the privacy loss of the mechanism based on the information
maintained by the privacy accountant.The accountant procedure computes the privacy cost at each access to the training data;
Privacy loss is a random variable dependent on the random noise added to the algorithm;
- implementation:
+ with Python and TensorFlow
+ main component: PrivacyAccountant which keeps track of privacy spending over the course of training.
+ moments accountant: mechansim for tracking privacy loss
+ algorithms are based on a differentially private version of stochastic gradient descent
- application:
to image datasets (MNIST & CIFAR-10)
---
(D3)
title: One size does not fit all: Investigating strategies for differentially-private
learning across NLP tasks
author(s): Senge et al.
year: 2021
abstract: Preserving privacy in training modern NLP
models comes at a cost. We know that
stricter privacy guarantees in differentiallyprivate
stochastic gradient descent (DP-SGD)
generally degrade model performance. However,
previous research on the efficiency of DPSGD
in NLP is inconclusive or even counterintuitive.
In this short paper, we provide a
thorough analysis of different privacy preserving
strategies on seven downstream datasets in
five different ‘typical’ NLP tasks with varying
complexity using modern neural models.
We show that unlike standard non-private approaches
to solving NLP tasks, where bigger
is usually better, privacy-preserving strategies
do not exhibit a winning pattern, and each
task and privacy regime requires a special treatment
to achieve adequate performance.
bibtex:
@article{SengeManuel2022Osdn,
year = {2022},
title = {One size does not fit all: Investigating strategies for differentially-private learning across NLP tasks},
copyright = {2022. This work is published under http://creativecommons.org/licenses/by-sa/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
language = {eng},
address = {Ithaca},
author = {Senge, Manuel and Igamberdiev, Timour and Habernal, Ivan},
keywords = {Performance degradation ; Privacy ; Task complexity},
issn = {2331-8422},
abstract = {Preserving privacy in contemporary NLP models allows us to work with sensitive data, but unfortunately comes at a price. We know that stricter privacy guarantees in differentially-private stochastic gradient descent (DP-SGD) generally degrade model performance. However, previous research on the efficiency of DP-SGD in NLP is inconclusive or even counter-intuitive. In this short paper, we provide an extensive analysis of different privacy preserving strategies on seven downstream datasets in five different `typical' NLP tasks with varying complexity using modern neural models based on BERT and XtremeDistil architectures. We show that unlike standard non-private approaches to solving NLP tasks, where bigger is usually better, privacy-preserving strategies do not exhibit a winning pattern, and each task and privacy regime requires a special treatment to achieve adequate performance.},
journal = {arXiv.org},
publisher = {Cornell University Library, arXiv.org},
}
content:
- for experiments: 7 widely-used datasets covering 5 standard NLP tasks (sentiment analysis, sequence tagging (NER and POS), text classification, question-answering)
- privacy budget: is an upper bound on the privacy cost, where a smaller pb guarantees stronger privacy;
when exposed to DP, for NER only the most common tags are well predicted (outside tag)
-results: skewed class distribution hurts the perfromacne with DP-SGD
---
(D4)
title: Training Text-to-Text Transformers with Privacy Guarantees
author(s): Ponomareva et al.
year: 2022
abstract: Recent advances in NLP often stem from large
transformer-based pre-trained models, which
rapidly grow in size and use more and more
training data. Such models are often released
to the public so that end users can fine-tune
them on a task dataset. While it is common
to treat pre-training data as public, it may
still contain personally identifiable information
(PII), such as names, phone numbers, and
copyrighted material. Recent findings show
that the capacity of these models allows them
to memorize parts of the training data, and suggest
differentially private (DP) training as a potential
mitigation. While there is recent work
on DP fine-tuning of NLP models, the effects
of DP pre-training are less well understood: it
is not clear how downstream performance is
affected by DP pre-training, and whether DP
pre-training mitigates some of the memorization
concerns. We focus on T5 and show that
by using recent advances in JAX and XLA we
can train models with DP that do not suffer a
large drop in pre-training utility, nor in training
speed, and can still be fine-tuned to high
accuracy on downstream tasks (e.g. GLUE).
Moreover, we show that T5’s span corruption
is a good defense against data memorization.
bibtex:
@inproceedings{ponomareva-etal-2022-training,
    title = "Training Text-to-Text Transformers with Privacy Guarantees",
    author = "Ponomareva, Natalia  and
      Bastings, Jasmijn  and
      Vassilvitskii, Sergei",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.171",
    doi = "10.18653/v1/2022.findings-acl.171",
    pages = "2182--2193",
    abstract = "Recent advances in NLP often stem from large transformer-based pre-trained models, which rapidly grow in size and use more and more training data. Such models are often released to the public so that end users can fine-tune them on a task dataset. While it is common to treat pre-training data as public, it may still contain personally identifiable information (PII), such as names, phone numbers, and copyrighted material. Recent findings show that the capacity of these models allows them to memorize parts of the training data, and suggest differentially private (DP) training as a potential mitigation. While there is recent work on DP fine-tuning of NLP models, the effects of DP pre-training are less well understood: it is not clear how downstream performance is affected by DP pre-training, and whether DP pre-training mitigates some of the memorization concerns. We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE). Moreover, we show that T5{'}s span corruption is a good defense against data memorization.",
}
content:
- they explore how well DP mitigates privacy risks and
how it affects pre-training and downstream performance.
term "private": a model can be described as private if it is robust
to membership attacks, training data extraction attacks,
or to attacks that attempt to infer some private
attribute (e.g., the race of a speaker) from the data.
- method: private pre-training and public fine-tuning; DP training - DP training is usually
achieved via gradient noise or perturbing the loss.
- aim : verify whether DP pretraining can mitigate some privacy risks
- model: T5 small
- dataset: The Colossal Clean Crawled Corpus, C4
- training objective: predict next tokens given the context 
- metric for gauging memorization: exact match, token accuracy, token-level accuracy and median edit distance
- results: better pre-training utility does not directly translate into better downstream
fine-tuning performance; The take-away message here is that if memorization is of a concern, one
way to address it is to use span corruption training objective;
###(E) Knowledge Databases ###
(E1)
title: Non-named Entities - The Silent Majority
auhtor(s): Paris & Suchanek 
year: 2021
abstract: Knowledge Bases (KBs) usually contain named entities.
However, the majority of entities in natural language text are not named.
In this position paper, we first study the nature of these entities. Then
we explain how they could be represented in KBs. Finally, we discuss
open challenges for adding non-named entities systematically to KBs.
- content:
+ definition of a noun phrase and named entity
+ overview of modeling of entities in various KBs(DBPedia, Yago etc.)
---
(E2)
title: YAGO 4: A Reason-able Knowledge Base
author(s): Tanon et al.
year: 2020
abstract: YAGO is one of the large knowledge bases in the Linked
Open Data cloud. In this resource paper, we present its latest version,
YAGO 4, which reconciles the rigorous typing and constraints of
schema.org with the rich instance data of Wikidata. The resulting resource
contains 2 billion type-consistent triples for 64 Million entities,
and has a consistent ontology that allows semantic reasoning with OWL 2
description logics.
- content:
+ YAGO 4 combines Wikidata and schema.org
+ human-readable URIs for all entities (e.g., Bischmisheim_Q866094).
+ YAGO 4 is made available in three "flavors":
a) Full --> all data from wikidata
b) Wikipedia --> only instances that have a wikipedia article
c) English Wikipedia --> only instances that have an English wikipedia article
+ YAGO 4 is available under
a Creative Commons Attribution-ShareAlike License.

###(F) DATA PRIVACY###
(F1)
title:Privacy Dictionary: A New Resource for the Automated Content Analysis of Privacy
year: 2011
bibtex: @article{VasalouAsimina2011PdAn,
year = {2011},
title = {Privacy dictionary: A new resource for the automated content analysis of privacy},
copyright = {2011 ASIS&T},
language = {eng},
address = {Hoboken},
author = {Vasalou, Asimina and Gill, Alastair J. and Mazanderani, Fadhila and Papoutsi, Chrysanthi and Joinson, Adam},
keywords = {Automation ; Computational linguistics ; Computer Science ; Computer Science Information Systems ; Content analysis ; Corpus linguistics ; Definitions ; Dictionaries ; Empirical analysis ; Exact sciences and technology ; Information and communication sciences ; Information Science & Library Science ; Information science. Documentation ; Internet resources ; Language patterns ; Library and information science. General aspects ; Linguistics ; Privacy ; Science & Technology ; Sciences and techniques of general use ; Social sciences ; Software ; Studies ; Technology ; Theoretical linguistics ; Use and user studies. Information needs},
issn = {1532-2882},
abstract = {This article presents the privacy dictionary, a new linguistic resource for automated content analysis on privacy‐related texts. To overcome the definitional challenges inherent in privacy research, the dictionary was informed by an inclusive set of relevant theoretical perspectives. Using methods from corpus linguistics, we constructed and validated eight dictionary categories on empirical material from a wide range of privacy‐sensitive contexts. It was shown that the dictionary categories are able to measure unique linguistic patterns within privacy discussions. At a time when privacy considerations are increasing and online resources provide ever‐growing quantities of textual data, the privacy dictionary can play a significant role not only for research in the social sciences but also in technology design and policymaking.},
journal = {Journal of the American Society for Information Science and Technology},
pages = {2095--2105},
volume = {62},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
number = {11},
}
content:
+ p.2095: Understanding individuals’ privacy perceptions, particularly in relation to technology, thus has become a central question
that cuts across a number of disciplines (e.g., human– computer interaction, information science, communication studies, computer science).
+ p.2096: privacy dictionary for automated content analysis to allow researchers to systematically measure different aspects and uses of privacy language.
---
(F2)
title: Understanding the Capabilities, Limitations, and
Societal Impact of Large Language Models
year: 2021
bibtex:
@article{TamkinAlex2021UtCL,
year = {2021},
title = {Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Tamkin, Alex and Brundage, Miles and Clark, Jack and Ganguli, Deep},
abstract = {On October 14th, 2020, researchers from OpenAI, the Stanford Institute for
Human-Centered Artificial Intelligence, and other universities convened to
discuss open research questions surrounding GPT-3, the largest
publicly-disclosed dense language model at the time. The meeting took place
under Chatham House Rules. Discussants came from a variety of research
backgrounds including computer science, linguistics, philosophy, political
science, communications, cyber policy, and more. Broadly, the discussion
centered around two main questions: 1) What are the technical capabilities and
limitations of large language models? 2) What are the societal effects of
widespread use of large language models? Here, we provide a detailed summary of
the discussion organized by the two themes above.},
}
content:
+ discussion among experts around two central questions:
a) What are the technical capabilities and limitations of large language models?
b) What are the societal effects of widespread use of large language models?
+ LLMs improve performance with scale
+ future research: What kinds of tests do we
need to develop in order to qualify language models like GPT-3 as being
safe or unsafe for use in particular contexts?
---
(F3)
title: Exposed! A Survey of Attacks on Private Data
year: 2017
bibtex:
@article{DworkCynthia2017EASo,
year = {2017},
title = {Exposed! A Survey of Attacks on Private Data},
copyright = {Copyright © 2017 by Annual Reviews. All rights reserved 2017},
language = {eng},
author = {Dwork, Cynthia and Smith, Adam and Steinke, Thomas and Ullman, Jonathan},
keywords = {differential privacy ; privacy ; privacy attacks ; re-identification ; reconstruction attacks ; tracing attacks},
issn = {2326-8298},
abstract = {Privacy-preserving statistical data analysis addresses the general question of protecting privacy when publicly releasing information about a sensitive dataset. A privacy attack takes seemingly innocuous released information and uses it to discern the private details of individuals, thus demonstrating that such information compromises privacy. For example, re-identification attacks have shown that it is easy to link supposedly de-identified records to the identity of the individual concerned. This survey focuses on attacking aggregate data, such as statistics about how many individuals have a certain disease, genetic trait, or combination thereof. We consider two types of attacks: reconstruction attacks, which approximately determine a sensitive feature of all the individuals covered by the dataset, and tracing attacks, which determine whether or not a target individual's data are included in the dataset. We also discuss techniques from the differential privacy literature for releasing approximate aggregate statistics while provably thwarting any privacy attack.},
journal = {Annual review of statistics and its application},
pages = {61--84},
volume = {4},
publisher = {Annual Reviews},
number = {1},
}
content:
+ three adversarial goals:
a) p.12.3: Re-identification/de-anonymization: Re-identification refers to reversing this step, tracing an individual record back to its human source.
b) p.12.3-4: Reconstruction: The goal in a reconstruction attack is to determine the secret bits for nearly all individuals in the dataset.
c) p.12.4: Tracing: determining whether or not a specific individual is a member of a given dataset;
+ p.12.5: differential privacy by definition prevents tracing; it also protects against reconstruction and re-identification.
+ p.12.5-6: description of recostruction attacks
+ p.12.11: mere presence in the dataset can be highly sensitive information.
+ p.12.10-17: description of tracing attack (~membership inference attack)
+ p.12.17-18: discussion on differential privacy;
---
(F4)
title: Privacy risk in machine learning
year: 2018
bibtex:
@INPROCEEDINGS{yeom_2018,
  author={Yeom, Samuel and Giacomelli, Irene and Fredrikson, Matt and Jha, Somesh},
  booktitle={2018 IEEE 31st Computer Security Foundations Symposium (CSF)}, 
  title={Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting}, 
  year={2018},
  volume={},
  number={},
  pages={268-282},
  doi={10.1109/CSF.2018.00027}}
content:
+ p.268: leak information about that data through either their behavior (i.e., black-box attack) or the details of their structure (i.e., white-box attack).
+ two factors for privacy risk: overfitting and influence;
+ Membership inference aim to determine whether a given data point was present in the training data used to build a model.
+ p.269: In an attribute inference attack, the adversary uses a machine learning model and incomplete
information about a data point to infer the missing information for that point.
+ aim: This paper explores the relationships between
privacy, overfitting, and influence in machine learning models
+ result: machine learning models can pose immediate threats to privacy without overfitting
+ p.270, 280: In a membership inference attack, the adversary attempts to infer whether a specific point was included in the dataset used
to train a given model. The adversary is given a data point z = (x, y), access to a model AS, the size of the model’s
training set |S| = n, and the distribution D that the training set was drawn from.
+ p.270-1: membership advantage
+ p.273: despite their remarkably good generalization error, deep CNNs may have the capacity to effectively “memorize” the dataset.
+ p.280-1: results
---
(F5)
title:When Machine Learning Meets Privacy: A Survey and Outlook
year: 2021
bibtex:
@article{liu_2021,
author = {Liu, Bo and Ding, Ming and Shaham, Sina and Rahayu, Wenny and Farokhi, Farhad and Lin, Zihuai},
title = {When Machine Learning Meets Privacy: A Survey and Outlook},
year = {2021},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3436755},
doi = {10.1145/3436755},
abstract = {The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {31},
numpages = {36},
keywords = {Machine learning, differential privacy, deep learning, privacy}
}
content:
+ p.31:2: emphasis on mitigating privacy risks during the machine learning process by paying special attention to the privacy challenges and
risks associated with the ML models.
+ p.31:3: description of supervised ML model;
+ p.31:4-5: distinction centralized vs. distributed learning; centralized learning is characterized by “globally stored data” and “globally trained
model,” as shown in Figure 1(a), while the distributed learning is characterized by “locally stored data” and “locally trained model,” as shown in Figure 1(b).
+ p.31:8: distinction white-box vs. black-box access
+ p.31:10: membership inference attack
+ p.31:11-14: private machine learning schemes
a) encryption
b) obfuscation
c) aggregeation
---

###(O) BERT ###
title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
auhtor(s): Devlin et al.
year: 2018
bibtex:
@article{DevlinJacob2018BPoD,
year = {2018},
title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
language = {eng},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
keywords = {Computer Science - Computation and Language},
abstract = {We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).},
}
content:
- code and pre-trained models available at https://github.com/google-research/bert (see tokenization!)
- notes:
+ problem? English Wikipedia is used as corpus in pre-training of BERT
+ glue benchmark: https://gluebenchmark.com/leaderboard
+ no real NER task in pre-training or fine-tuning of BERT
